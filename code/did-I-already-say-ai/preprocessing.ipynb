{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers tqdm pandas datasets matplotlib\n",
    "\n",
    "# Imports\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset to pandas DataFrame...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383775bdb7f643a68fad1baf2155ba1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering AI posts: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset to pandas DataFrame...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ai_keywords = [\n",
    "    \"artificial intelligence\", \"ai\", \"machine learning\", \"deep learning\", \"neural network\",\n",
    "    \"openai\", \"large language model\", \"llm\", \"transformer model\",\n",
    "    \"natural language processing\", \"nlp\", \"automation\", \"robots\", \"robotics\", \"autonomous\",\n",
    "    \"reinforcement learning\", \"generative model\", \"text generation\", \"image generation\", \"mlp\"\n",
    "]\n",
    "pattern = re.compile(r'\\b(' + '|'.join(map(re.escape, ai_keywords)) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "\n",
    "stream = load_dataset(\"webis/tldr-17\", split=\"train\", streaming=True)\n",
    "filtered_rows = []\n",
    "max_matches = 50000\n",
    "MAX = 1_400_000\n",
    "print(\"Converting dataset to pandas DataFrame...\")\n",
    "for i, ex in enumerate(tqdm(stream, desc=\"Filtering AI posts\")):\n",
    "    # if i >= MAX:\n",
    "    #     print(\"Hit maximum expected dataset size. Stopping.\")\n",
    "    #     break\n",
    "    text = ex['normalizedBody']\n",
    "    if pattern.search(text):\n",
    "        filtered_rows.append({\n",
    "            \"normalizedBody\": ex[\"normalizedBody\"],\n",
    "            \"subreddit\": ex[\"subreddit\"],\n",
    "            \"subreddit_id\": ex[\"subreddit_id\"],\n",
    "            \"content\": ex[\"content\"],\n",
    "            \"summary\": ex[\"summary\"]\n",
    "        })\n",
    "    if len(filtered_rows) >= max_matches:\n",
    "        print(f\"Reached maximum matches: {max_matches}. Stopping.\")\n",
    "        break\n",
    "print(\"Converting dataset to pandas DataFrame...\")\n",
    "# Convert to pandas DataFrame\n",
    "df_ai = pd.DataFrame(filtered_rows)\n",
    "\n",
    "df_ai = df_ai[[\"normalizedBody\", \"subreddit\", \"subreddit_id\", \"content\", \"summary\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_ai.to_parquet(\"ai_posts.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AI', 17446), ('robots', 5536), ('ai', 2860), ('automation', 2854), ('MLP', 2687), ('robotics', 1666), ('autonomous', 1528), ('artificial intelligence', 578), ('machine learning', 503), ('Robots', 501), ('Robotics', 466), ('Ai', 367), ('NLP', 325), ('Automation', 319), ('mlp', 300), ('neural network', 211), ('Artificial Intelligence', 185), ('Autonomous', 117), ('Machine Learning', 112), ('LLM', 88), ('deep learning', 57), ('natural language processing', 41), ('ROBOTS', 40), ('Artificial intelligence', 30), ('Machine learning', 30), ('reinforcement learning', 19), ('nlp', 16), ('Deep Learning', 16), ('aI', 13), ('Deep learning', 11), ('Natural Language Processing', 10), ('Reinforcement Learning', 6), ('Mlp', 6), ('Neural Network', 6), ('AUTOMATION', 5), ('artificial Intelligence', 4), ('image generation', 4), ('Natural language processing', 4), ('Neural network', 4), ('llm', 3), ('text generation', 2), ('AUTONOMOUS', 2), ('generative model', 2), ('Reinforcement learning', 1), ('ARTIFICIAL Intelligence', 1), ('DEEP Learning', 1), ('LlM', 1), ('Nlp', 1), ('MACHINE LEARNING', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter()\n",
    "\n",
    "for text in df_ai[\"content\"]:\n",
    "    matches = pattern.findall(str(text))\n",
    "    word_counts.update(matches)\n",
    "\n",
    "print(word_counts.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23987"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ai = pd.read_parquet(\"ai_posts.parquet\")\n",
    "df_ai.head(10)\n",
    "len(df_ai)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docAnaProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
